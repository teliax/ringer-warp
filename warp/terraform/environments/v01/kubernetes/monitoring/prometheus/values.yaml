# kube-prometheus-stack values for WARP monitoring
fullnameOverride: "warp-monitoring"
defaultRules:
  create: true
  rules:
    alerting: true
    etcd: true
    general: true
    k8s: true
    kubeApiserver: true
    kubeApiserverAvailability: true
    kubeApiserverError: true
    kubeApiserverSlos: true
    kubelet: true
    kubePrometheusGeneral: true
    kubePrometheusNodeAlerting: true
    kubePrometheusNodeRecording: true
    kubeScheduler: true
    kubeStateMetrics: true
    network: true
    node: true
    prometheus: true
    prometheusOperator: true
    
alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default-receiver'
      routes:
      - match:
          severity: critical
        receiver: critical-receiver
        continue: true
      - match:
          service: kamailio
        receiver: sip-team
      - match:
          service: rtpengine
        receiver: media-team
    receivers:
    - name: 'default-receiver'
      slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK_URL'
        channel: '#warp-alerts'
        title: 'WARP Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}'
    - name: 'critical-receiver'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}'
    - name: 'sip-team'
      email_configs:
      - to: 'sip-oncall@warp.com'
        headers:
          Subject: 'SIP Service Alert: {{ .GroupLabels.alertname }}'
    - name: 'media-team'
      email_configs:
      - to: 'media-oncall@warp.com'
        headers:
          Subject: 'Media Service Alert: {{ .GroupLabels.alertname }}'
  
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: standard
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

grafana:
  enabled: true
  defaultDashboardsEnabled: true
  adminPassword: "ChangeThisPassword123!"
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      - grafana.warp-monitoring.local
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.warp-monitoring.local
  
  additionalDataSources:
    - name: Loki
      type: loki
      access: proxy
      url: http://loki:3100
      jsonData:
        maxLines: 1000
    - name: Jaeger
      type: jaeger
      access: proxy
      url: http://jaeger-query:16686
      
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default
      - name: 'warp'
        orgId: 1
        folder: 'WARP'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/warp
  
  dashboardsConfigMaps:
    default: "grafana-dashboards-default"
    warp: "grafana-dashboards-warp"
  
  grafana.ini:
    server:
      root_url: "https://grafana.warp-monitoring.local"
    auth:
      disable_login_form: false
    auth.generic_oauth:
      enabled: true
      name: Google
      allow_sign_up: true
      client_id: YOUR_OAUTH_CLIENT_ID
      client_secret: $__env{GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET}
      scopes: openid email profile
      auth_url: https://accounts.google.com/o/oauth2/auth
      token_url: https://accounts.google.com/o/oauth2/token
      api_url: https://www.googleapis.com/oauth2/v1/userinfo
    analytics:
      reporting_enabled: false
      check_for_updates: false
  
  envFromSecret: "grafana-oauth-secret"
      
prometheus:
  enabled: true
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector: {}
    serviceMonitorNamespaceSelector: {}
    podMonitorSelectorNilUsesHelmValues: false
    podMonitorSelector: {}
    podMonitorNamespaceSelector: {}
    ruleSelectorNilUsesHelmValues: false
    ruleSelector: {}
    ruleNamespaceSelector: {}
    
    retention: 30d
    retentionSize: "100GB"
    
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: standard
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 100Gi
    
    resources:
      requests:
        cpu: 500m
        memory: 2Gi
      limits:
        cpu: 2
        memory: 8Gi
    
    additionalScrapeConfigs:
    # Kamailio metrics
    - job_name: 'kamailio'
      static_configs:
      - targets:
        - 'kamailio-svc.default.svc.cluster.local:9090'
      metrics_path: '/metrics'
      
    # RTPEngine metrics
    - job_name: 'rtpengine'
      static_configs:
      - targets:
        - 'rtpengine-svc.default.svc.cluster.local:9091'
      metrics_path: '/metrics'
      
    # Consul metrics
    - job_name: 'consul'
      consul_sd_configs:
      - server: 'consul-server.default.svc.cluster.local:8500'
      relabel_configs:
      - source_labels: [__meta_consul_service]
        regex: '(.*)'
        target_label: consul_service
        
    # Redis metrics via exporter
    - job_name: 'redis'
      static_configs:
      - targets:
        - 'redis-exporter.default.svc.cluster.local:9121'
        
    # PostgreSQL metrics via exporter  
    - job_name: 'postgres'
      static_configs:
      - targets:
        - 'postgres-exporter.default.svc.cluster.local:9187'

prometheusOperator:
  resources:
    limits:
      cpu: 200m
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 100Mi

kubeStateMetrics:
  enabled: true

nodeExporter:
  enabled: true

# Additional components to monitor
coreDns:
  enabled: true

kubeControllerManager:
  enabled: true

kubeEtcd:
  enabled: true

kubeScheduler:
  enabled: true

kubeProxy:
  enabled: true

kubeApiServer:
  enabled: true